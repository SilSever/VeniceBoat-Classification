{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset_manipulation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "x8jZG4C5GhVk",
        "colab_type": "code",
        "outputId": "241bc96e-ced5-4140-c55b-15bd60e4a72f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://SilSever:Milano15!@github.com/SilSever/VeniceBoatDataset.git"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'VeniceBoatDataset' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xislrtoqGPx3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#First\n",
        "#import tensorflow as tf\n",
        "#import tensorflow.contrib.eager as tfe\n",
        "#tf.enable_eager_execution()\n",
        "\n",
        "\n",
        "#Second\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.python.keras.layers.convolutional import Conv2D\n",
        "from tensorflow.python.keras.layers.convolutional import MaxPooling2D\n",
        "from tensorflow.python.keras.layers.convolutional import ZeroPadding2D\n",
        "from tensorflow.python.keras.layers.convolutional import Convolution2D\n",
        "from tensorflow.python.keras.layers.core import Activation\n",
        "from tensorflow.python.keras.layers.core import Flatten\n",
        "from tensorflow.python.keras.layers.core import Dropout\n",
        "from tensorflow.python.keras.layers.core import Dense\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "#Third\n",
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "#matplotlib.use(\"Agg\")\n",
        " \n",
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.python.keras.optimizers import Adam\n",
        "from tensorflow.python.keras.preprocessing.image import img_to_array\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.python.keras.models import load_model\n",
        "import imutils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j90QaCjFGPyh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Preprocessing CNN"
      ]
    },
    {
      "metadata": {
        "id": "eRqQPpVzGPyi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initialize the number of epochs to train for, initial learning rate,\n",
        "# batch size, and image dimensions\n",
        "EPOCHS = 50\n",
        "INIT_LR = 1e-3\n",
        "BS = 32\n",
        "IMAGE_DIMS = (180, 180, 3) #Height x Width x RGB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BSv4h2v9GPyn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from VeniceBoatDataset.dataset_manipulation import read_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "NIiq9IQhGPyp",
        "colab_type": "code",
        "outputId": "bfa860a6-fb4a-47fc-e253-4565a86dc23e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#depth:\n",
        "# -1 image directory\n",
        "# -2 boats classification\n",
        "# -3 family classification\n",
        "# -4 source path\n",
        "data, labels = read_images('VeniceBoat-Dataset/sc5-tensorflow', 2, IMAGE_DIMS[0], IMAGE_DIMS[1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zXITFT373WcW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class_name = list(set(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w5Pzn8OkGPyw",
        "colab_type": "code",
        "outputId": "4aa79fad-8c7a-478b-e72d-ad1a6c082c7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# scale the raw pixel intensities to the range [0, 1]\n",
        "data = np.array(data, dtype=\"float\") / 255.0\n",
        "labels = np.array(labels)\n",
        "print(\"[INFO] data matrix: {:.2f}MB\".format(\n",
        "    data.nbytes / (1024 * 1000.0)))\n",
        " \n",
        "# binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        " \n",
        "# partition the data into training and testing splits using 80% of\n",
        "# the data for training and the remaining 20% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data,\n",
        "    labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# construct the image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
        "    height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
        "    horizontal_flip=True, fill_mode=\"nearest\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] data matrix: 3617.66MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uSntHNAT3aMH",
        "colab_type": "code",
        "outputId": "77e2b213-7ce0-4da6-d773-7be43b631099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "testX.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(953, 180, 180, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "hFxVTZ-YGPy2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training model"
      ]
    },
    {
      "metadata": {
        "id": "dc0nQKt-GPy2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initialize the model\n",
        "print(\"[INFO] compiling model...\")\n",
        "model = SmallerVGGNet.build(width=IMAGE_DIMS[1], height=IMAGE_DIMS[0],depth=IMAGE_DIMS[2], classes=labels.shape[1])\n",
        "#model = VGG_16(IMAGE_DIMS[0], IMAGE_DIMS[1], IMAGE_DIMS[2], labels.shape[1])\n",
        "#opt = Adam(lr=INIT_LR, decay=INIT_LR/EPOCHS)\n",
        "#opt = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
        "opt = tf.train.AdamOptimizer(learning_rate=INIT_LR)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\", f1])\n",
        "# train the network\n",
        "print(\"[INFO] training network...\")\n",
        "H = model.fit_generator( aug.flow(trainX, trainY, batch_size=BS),\n",
        "                          validation_data=(testX, testY),\n",
        "                          steps_per_epoch=len(trainX) // BS,\n",
        "                          epochs=EPOCHS, \n",
        "                          verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "85vLzzcUGPy5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# save the model to disk\n",
        "print(\"[INFO] serializing network...\")\n",
        "model.save('VeniceBoat-Dataset/model')\n",
        " \n",
        "# save the label binarizer to disk\n",
        "print(\"[INFO] serializing label binarizer...\")\n",
        "f = open('VeniceBoat-Dataset/label', \"wb\")\n",
        "f.write(pickle.dumps(lb))\n",
        "f.close()\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "N = EPOCHS\n",
        "plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"f1\"], label=\"f1_score\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_f1\"], label=\"val_f1_score\")\n",
        "plt.title(\"Training Accuracy and F1 score\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Accuracy/f1\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('VeniceBoat-Dataset/plot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t-nziQcsGPy8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing model"
      ]
    },
    {
      "metadata": {
        "id": "F71M_BliGPy9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the image\n",
        "path = 'VeniceBoat-Dataset/sc5-test-tensorflow/Pleasurecraft/Topa/'\n",
        "images = os.listdir(path)\n",
        "im_path = path + images[0]\n",
        "\n",
        "image = cv2.imread(im_path)\n",
        "output = image.copy()\n",
        "\n",
        "\n",
        "# pre-process the image for classification\n",
        "image = cv2.resize(image, (180, 180))\n",
        "image = image.astype(\"float\") / 255.0\n",
        "image = img_to_array(image)\n",
        "image = np.expand_dims(image, axis=0)\n",
        "\n",
        "\n",
        "# load the trained convolutional neural network and the label\n",
        "# binarizer\n",
        "print(\"[INFO] loading network...\")\n",
        "model = load_model('VeniceBoat-Dataset/model-family')\n",
        "lb = pickle.loads(open('VeniceBoat-Dataset/label-family', \"rb\").read())\n",
        " \n",
        "# classify the input image\n",
        "print(\"[INFO] classifying image...\")\n",
        "\n",
        "proba = model.predict(image)[0]\n",
        "idx = np.argmax(proba)\n",
        "label = lb.classes_[idx]\n",
        "\n",
        "label = \"{}: {:.2f}% ({})\".format(label, proba[idx] * 100, 'correct')\n",
        "print(\"[INFO] {}\".format(label))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}